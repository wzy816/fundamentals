{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "Models that assign probabilities to sequences of words, aka LM.\n",
    "\n",
    "Tasks include:\n",
    "- compute the probability of a sentence $P(W)$\n",
    "- compute the probability of a word given several words $P(W_i|W_1W_2 \\dots W_n)$\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "## Chain Rule\n",
    "\n",
    "$P(W_1 W_2 \\dots W_k) = P(W_1) P(W_2|W_1) P(W_3|W_1 W_2) \\dots P(W_k | W_1 W_2 \\dots W_{k-1})$\n",
    "\n",
    "## Markov Assumption\n",
    "\n",
    "The probability of a word depends only on the previous word.\n",
    "\n",
    "$ P(W_i|W_1 \\dots W_i-1) = P(W_i | W_{i-n+1}| \\dots W_{i-1})$\n",
    "\n",
    "## N-gram model\n",
    "\n",
    "### 1. Unigram Model\n",
    "\n",
    "The probability of a sentence is the product of probabilities of all words.\n",
    "\n",
    "Words are independent. \n",
    "\n",
    "$P(W)=\\prod_{i}P(W_i)$\n",
    "\n",
    "### 2. Bigram Model\n",
    "\n",
    "Word depends on its previous one word.\n",
    "\n",
    "$P(W_i | W_1 W_2 \\dots W_{i-1}) = P(W_i | W _{i-1})$\n",
    "\n",
    "### tri-gram, 4-gram, 5-gram ...\n",
    "\n",
    "## Flaws\n",
    "\n",
    "- Language has long-distance dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算字符串编辑距离 edit distance\n",
    "\n",
    "Smith-Waterman algorithm\n",
    "\n",
    "Non-negative socre is suitable for local alignment.\n",
    "\n",
    "$ H_{ij} = max \\left \\{\n",
    "  \\begin{aligned}\n",
    "    H_{i-1,j-1} + S_{i,j} \\\\\n",
    "    max_{k>=1} \\{ H_{i-k,j} - W_k\\} \\\\\n",
    "    max_{l>=1} \\{ H_{i,j-l} - W_l\\} \\\\\n",
    "    0\n",
    "  \\end{aligned} \\right.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smith_waterman(list_1, list_2):\n",
    "    def substitution(a,b):\n",
    "        if a == b:\n",
    "            return 3\n",
    "        else: \n",
    "            return -3\n",
    "\n",
    "    def penalty(k):\n",
    "        return 2*k\n",
    "\n",
    "    def score(h, i, j):\n",
    "        match = h[i-1,j-1]+substitution(list_1[i-1], list_2[j-1])\n",
    "        delete = max([h[i-k,j]-penalty(k) for k in range(0,i)])\n",
    "        insert = max([h[i,j-l]-penalty(l) for l in range(0,j)])\n",
    "        return max(match,delete,insert,0)\n",
    "    \n",
    "    # init\n",
    "    n = len(list_1)\n",
    "    m = len(list_2)\n",
    "    H = np.zeros((n+1,m+1))\n",
    "    H[:, 0] = 0\n",
    "    H[0, :] = 0\n",
    "\n",
    "    # fill score matrix   \n",
    "    i = j = 1\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            H[i,j] = score(H, i, j)\n",
    "    print(H)\n",
    "    \n",
    "    # backtrace\n",
    "    H_flip = np.flip(np.flip(H, 0), 1)\n",
    "    start_i, start_j = np.unravel_index(np.argmax(H_flip, axis=None), H_flip.shape)\n",
    "    start_i = n - start_i\n",
    "    start_j = m - start_j\n",
    "    \n",
    "    def pair(i,j):\n",
    "        return {'i': i,'j': j,'pair': list_1[i-1] + ' - ' + list_2[j-1]}\n",
    "    \n",
    "    path = []\n",
    "    while H[start_i, start_j] > 0:\n",
    "        path.append(pair(start_i,start_j))\n",
    "        v1 = H[start_i-1, start_j]\n",
    "        v2 = H[start_i, start_j-1]\n",
    "        v3 = H[start_i-1, start_j-1]\n",
    "        if v1 >= v2 and v1 >= v3:\n",
    "            start_i = start_i - 1\n",
    "        elif v2 >= v1 and v2 >= v3:\n",
    "            start_j = start_j - 1\n",
    "        elif v3 >= v1 and v3 >= v2:\n",
    "            start_i = start_i - 1\n",
    "            start_j = start_j - 1\n",
    "        \n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  3.  1.  0.  0.  0.  3.  3.  1.]\n",
      " [ 0.  0.  3.  1.  0.  0.  0.  3.  6.  4.]\n",
      " [ 0.  3.  1.  6.  4.  2.  0.  1.  4.  3.]\n",
      " [ 0.  3.  1.  4.  9.  7.  5.  3.  2.  1.]\n",
      " [ 0.  1.  6.  4.  7.  6.  4.  8.  6.  4.]\n",
      " [ 0.  0.  4.  3.  5. 10.  8.  6.  5.  3.]\n",
      " [ 0.  0.  2.  1.  3.  8. 13. 11.  9.  8.]\n",
      " [ 0.  3.  1.  5.  4.  6. 11. 10.  8.  6.]\n",
      " [ 0.  1.  0.  3.  2.  7.  9.  8.  7.  5.]]\n",
      "[{'i': 1, 'j': 2, 'pair': 'G - G'}, {'i': 2, 'j': 2, 'pair': 'G - G'}, {'i': 3, 'j': 3, 'pair': 'T - T'}, {'i': 4, 'j': 4, 'pair': 'T - T'}, {'i': 5, 'j': 4, 'pair': 'G - T'}, {'i': 6, 'j': 5, 'pair': 'A - A'}, {'i': 7, 'j': 6, 'pair': 'C - C'}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sw = smith_waterman([\"G\",\"G\",\"T\",\"T\",\"G\",\"A\",\"C\",\"T\",\"A\"], [\"T\",\"G\",\"T\",\"T\",\"A\",\"C\",\"G\",\"G\",\"C\"])\n",
    "print(sw,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needleman-Wunsch algorithm\n",
    "\n",
    "$ H_{ij} = max \\left \\{\n",
    "  \\begin{aligned}\n",
    "    H_{i-1,j-1} + S_{i,j} \\\\\n",
    "    H_{i-k,j} - W \\\\\n",
    "    H_{i-1,j} - W \\\\\n",
    "  \\end{aligned} \\right.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def needleman_wunsch(list_1,list_2):\n",
    "\n",
    "    def substitution(a,b):\n",
    "        if a == b:\n",
    "            return 1\n",
    "        else: \n",
    "            return -1\n",
    "    def penalty(k):\n",
    "        return k\n",
    "\n",
    "    # diff than smith_waterman\n",
    "    def score(h, i, j):\n",
    "        match = h[i-1,j-1]+substitution(list_1[i-1], list_2[j-1])\n",
    "        delete = h[i,j-1]-penalty(1)\n",
    "        insert = h[i-1,j]-penalty(1)\n",
    "        return max(match,delete,insert)\n",
    "\n",
    "    n = len(list_1)\n",
    "    m = len(list_2)\n",
    "    H = np.zeros((n+1,m+1),dtype=np.int32)\n",
    "    H[:, 0] = range(0,-n-1,-1)\n",
    "    H[0, :] = range(0,-m-1,-1)\n",
    "    \n",
    "    i = j = 1\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            H[i,j] = score(H, i, j)\n",
    "    print(H)\n",
    "\n",
    "    # backtrace\n",
    "    # might branch into diff paths\n",
    "    start_i = n\n",
    "    start_j = m\n",
    "    def pair(i,j):\n",
    "        return {'i': i,'j': j,'pair': list_1[i-1] + ' - ' + list_2[j-1]}\n",
    "    \n",
    "    path = []\n",
    "    while start_i >= 0 or start_j >= 0:\n",
    "        path.append(pair(start_i,start_j))\n",
    "        v1 = H[start_i-1, start_j]\n",
    "        v2 = H[start_i, start_j-1]\n",
    "        v3 = H[start_i-1, start_j-1]\n",
    "        if v1 >= v2 and v1 >= v3:\n",
    "            start_i = start_i - 1\n",
    "        elif v2 >= v1 and v2 >= v3:\n",
    "            start_j = start_j - 1\n",
    "        elif v3 >= v1 and v3 >= v2:\n",
    "            start_i = start_i - 1\n",
    "            start_j = start_j - 1\n",
    "        \n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10 -11]\n",
      " [ -1  -1   0  -1  -2  -3  -4  -5  -6  -7  -8  -9]\n",
      " [ -2  -2  -1  -1  -2  -1  -2  -3  -4  -5  -6  -7]\n",
      " [ -3  -1  -2  -2  -2  -2  -2  -3  -4  -3  -4  -5]\n",
      " [ -4  -2  -2  -1  -2  -3  -3  -1  -2  -3  -4  -3]\n",
      " [ -5  -3  -3  -1  -2  -3  -4  -2   0  -1  -2  -3]\n",
      " [ -6  -4  -4  -2  -2  -1  -2  -3  -1  -1   0  -1]\n",
      " [ -7  -5  -5  -3  -3  -2  -2  -3  -2   0  -1  -1]]\n",
      "[{'i': 0, 'j': 0, 'pair': 'C - T'}, {'i': 1, 'j': 1, 'pair': 'G - C'}, {'i': 1, 'j': 2, 'pair': 'G - G'}, {'i': 2, 'j': 3, 'pair': 'A - T'}, {'i': 2, 'j': 4, 'pair': 'A - G'}, {'i': 2, 'j': 5, 'pair': 'A - A'}, {'i': 3, 'j': 6, 'pair': 'C - A'}, {'i': 4, 'j': 7, 'pair': 'T - T'}, {'i': 5, 'j': 8, 'pair': 'T - T'}, {'i': 6, 'j': 9, 'pair': 'A - C'}, {'i': 6, 'j': 10, 'pair': 'A - A'}, {'i': 7, 'j': 11, 'pair': 'C - T'}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nw = needleman_wunsch([\"G\",\"A\",\"C\",\"T\",\"T\",\"A\",\"C\"], [\"C\",\"G\",\"T\",\"G\",\"A\",\"A\",\"T\",\"T\",\"C\",\"A\",\"T\"])\n",
    "print(nw,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram algorithm\n",
    "\n",
    "https://webdocs.cs.ualberta.ca/~kondrak/papers/spire05.pdf\n",
    "\n",
    "three variants: binary, comprehensive, positional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram edit distance\n",
    "# x, y are single characters\n",
    "# X, Y are listsa\n",
    "def d_single(x,y):\n",
    "    if x != None and y == None:\n",
    "        return 1\n",
    "    if x == None and y != None:\n",
    "        return 1\n",
    "    if x == y:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def d1(X,Y):    \n",
    "    k = len(X)\n",
    "    l = len(Y)\n",
    "    gamma = np.zeros((k+1,l+1))\n",
    "    gamma[:,0] = [i for i in range(k+1)]\n",
    "    gamma[0,:] = [j for j in range(l+1)]\n",
    "    for i in range(1,k+1):\n",
    "        for j in range(1,l+1):\n",
    "            gamma[i,j] = min(\n",
    "                gamma[i-1,j]+1,\n",
    "                gamma[i,j-1]+1,\n",
    "                gamma[i-1,j-1] + d_single(X[i-1],Y[j-1])\n",
    "            )\n",
    "    return gamma[k,l]\n",
    "\n",
    "# n-gram edit distance\n",
    "\n",
    "# 3 variants\n",
    "def positional(list_x, list_y):\n",
    "    n = len(list_x)\n",
    "    s = [d_single(list_x[i],list_y[i]) for i in range(n)]\n",
    "    return sum(s) / n\n",
    "\n",
    "def comprehensive(X, Y):\n",
    "    n = len(X)\n",
    "    return d1(X,Y) / n\n",
    "\n",
    "def binary(X,Y):\n",
    "    for i in range(len(X)):\n",
    "        if X[i] != Y[i]:\n",
    "            return 1\n",
    "    return 0\n",
    " \n",
    "def dn(X, Y, n, variant):\n",
    "    k = len(X)\n",
    "    l = len(Y)\n",
    "    X = [None]*(n-1) + X\n",
    "    Y = [None]*(n-1) + Y\n",
    "    D = np.empty((k+1,l+1))\n",
    "    D[:,0] = [i for i in range(k+1)]\n",
    "    D[0,:] = [j for j in range(l+1)]\n",
    "    for i in range(1,k+1):\n",
    "        for j in range(1,l+1):\n",
    "            D[i,j] = min(\n",
    "                D[i-1,j]+1,\n",
    "                D[i,j-1]+1,\n",
    "                D[i-1,j-1] + variant(X[i-1:i+n-1],Y[j-1:j+n-1])\n",
    "            )            \n",
    "    print(D)\n",
    "    return D[k,l] / max(k,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'b', 'c', 'd', 'e', 'f', 'a', 'r', 'j', 'n'] \n",
      "\n",
      "['a', 'b', 'c', 'd', 'e', 'a', 'w', 'e', 'q', 'g'] \n",
      "\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [ 1.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [ 2.  2.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [ 3.  3.  3.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [ 4.  4.  4.  4.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [ 5.  5.  5.  5.  5.  4.  5.  6.  7.  8.  9.]\n",
      " [ 6.  6.  6.  6.  6.  5.  5.  6.  7.  8.  9.]\n",
      " [ 7.  7.  7.  7.  7.  6.  6.  6.  7.  8.  9.]\n",
      " [ 8.  8.  8.  8.  8.  7.  7.  7.  7.  8.  9.]\n",
      " [ 9.  9.  9.  9.  9.  8.  8.  8.  8.  8.  9.]\n",
      " [10. 10. 10. 10. 10.  9.  9.  9.  9.  9.  9.]]\n",
      "binary \n",
      " 0.9\n",
      "[[ 0.    1.    2.    3.    4.    5.    6.    7.    8.    9.   10.  ]\n",
      " [ 1.    0.25  1.25  2.25  3.25  4.25  5.25  6.25  7.25  8.25  9.25]\n",
      " [ 2.    1.25  0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5 ]\n",
      " [ 3.    2.25  1.5   0.75  1.75  2.75  3.75  4.75  5.75  6.75  7.75]\n",
      " [ 4.    3.25  2.5   1.75  1.    2.    3.    4.    5.    6.    7.  ]\n",
      " [ 5.    4.25  3.5   2.75  2.    1.    2.    3.    4.    5.    6.  ]\n",
      " [ 6.    5.25  4.5   3.75  3.    2.    1.25  2.25  3.25  4.25  5.25]\n",
      " [ 7.    6.25  5.5   4.75  4.    3.    2.25  1.75  2.75  3.75  4.75]\n",
      " [ 8.    7.25  6.5   5.75  5.    4.    3.25  2.75  2.5   3.5   4.5 ]\n",
      " [ 9.    8.25  7.5   6.75  6.    5.    4.25  3.75  3.5   3.5   4.5 ]\n",
      " [10.    9.25  8.5   7.75  7.    6.    5.25  4.75  4.5   4.25  4.5 ]]\n",
      "comprehensive \n",
      " 0.45\n",
      "[[ 0.    1.    2.    3.    4.    5.    6.    7.    8.    9.   10.  ]\n",
      " [ 1.    0.25  1.25  2.25  3.25  4.25  5.25  6.25  7.25  8.25  9.25]\n",
      " [ 2.    1.25  0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5 ]\n",
      " [ 3.    2.25  1.5   0.75  1.75  2.75  3.75  4.75  5.75  6.75  7.75]\n",
      " [ 4.    3.25  2.5   1.75  1.    2.    3.    4.    5.    6.    7.  ]\n",
      " [ 5.    4.25  3.5   2.75  2.    1.    2.    3.    4.    5.    6.  ]\n",
      " [ 6.    5.25  4.5   3.75  3.    2.    1.25  2.25  3.25  4.25  5.25]\n",
      " [ 7.    6.25  5.5   4.75  4.    3.    2.25  1.75  2.75  3.75  4.75]\n",
      " [ 8.    7.25  6.5   5.75  5.    4.    3.25  2.75  2.5   3.5   4.5 ]\n",
      " [ 9.    8.25  7.5   6.75  6.    5.    4.25  3.75  3.5   3.5   4.5 ]\n",
      " [10.    9.25  8.5   7.75  7.    6.    5.25  4.75  4.5   4.25  4.5 ]]\n",
      "positional \n",
      " 0.45\n"
     ]
    }
   ],
   "source": [
    "l1 = [l for l in \"qbcdefarjn\"]\n",
    "l2 = [l for l in \"abcdeaweqg\"]\n",
    "n = 4\n",
    "print(l1,'\\n')\n",
    "print(l2,'\\n')\n",
    "\n",
    "bdn = dn(l1,l2,n,binary)\n",
    "print('binary \\n' ,bdn)\n",
    "\n",
    "cdn = dn(l1,l2,n,comprehensive)\n",
    "print('comprehensive \\n', cdn)\n",
    "\n",
    "pdn = dn(l1,l2,n,positional)\n",
    "print('positional \\n', pdn)\n",
    "\n",
    "# HAS BUG\n",
    "# cdn is always equal to pdn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "- an example of kn smoothing https://medium.com/@seccon/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8\n",
    "\n",
    "- kn library https://github.com/smilli/kneser-ney\n",
    "\n",
    "- good paper on all smoothing http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf\n",
    "\n",
    "- kn smoothing video https://www.youtube.com/watch?v=eNLUo3AIvcQ\n",
    "\n",
    "- textbook https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines \n",
      " [['paragraphs', 'are', 'the', 'building', 'blocks', 'of', 'papers'], ['many', 'students', 'define', 'paragraphs', 'in', 'terms', 'of', 'length', 'a', 'paragraph', 'is', 'a', 'group', 'of', 'at', 'least', 'five', 'sentences'], ['a', 'paragraph', 'is', 'half', 'a', 'page', 'long'], ['etc'], ['in', 'reality'], ['though'], ['the', 'unity', 'and', 'coherence', 'of', 'ideas', 'among', 'sentences', 'is', 'what', 'constitutes', 'a', 'paragraph'], ['a', 'paragraph', 'is', 'defined', 'as', 'a', 'group', 'of', 'sentences', 'or', 'a', 'single', 'sentence', 'that', 'forms', 'a', 'unit'], ['length', 'and', 'appearance', 'do', 'not', 'determine', 'whether', 'a', 'section', 'in', 'a', 'paper', 'is', 'a', 'paragraph'], ['for', 'instance'], ['in', 'some', 'styles', 'of', 'writing'], ['particularly', 'journalistic', 'styles'], ['a', 'paragraph', 'can', 'be', 'just', 'one', 'sentence', 'long'], ['ultimately'], ['a', 'paragraph', 'is', 'a', 'sentence', 'or', 'group', 'of', 'sentences', 'that', 'support', 'one', 'main', 'idea'], ['in', 'this', 'handout'], ['we', 'will', 'refer', 'to', 'this', 'as', 'the', 'controlling', 'idea'], ['because', 'it', 'controls', 'what', 'happens', 'in', 'the', 'rest', 'of', 'the', 'paragraph']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "SPLIT_PATTERN = r\"([!?.,;])\"\n",
    "WORD_PATTERN = r\"[a-zA-Z]+\"\n",
    "\n",
    "CORPUS = \"Paragraphs are the building blocks of papers. Many students define paragraphs in terms of length: a paragraph is a group of at least five sentences, a paragraph is half a page long, etc. In reality, though, the unity and coherence of ideas among sentences is what constitutes a paragraph. A paragraph is defined as “a group of sentences or a single sentence that forms a unit”. Length and appearance do not determine whether a section in a paper is a paragraph. For instance, in some styles of writing, particularly journalistic styles, a paragraph can be just one sentence long. Ultimately, a paragraph is a sentence or group of sentences that support one main idea. In this handout, we will refer to this as the “controlling idea,” because it controls what happens in the rest of the paragraph.\"\n",
    "\n",
    "lines = [] \n",
    "\n",
    "\n",
    "# tokenize\n",
    "for t in re.split(SPLIT_PATTERN, CORPUS):\n",
    "    l = re.findall(WORD_PATTERN,t.lower())\n",
    "    if len(l)>0:\n",
    "        lines.append(l)\n",
    "\n",
    "print('lines \\n', lines, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table1Gram \n",
      " {'paragraphs': 2, 'are': 1, 'the': 5, 'building': 1, 'blocks': 1, 'of': 8, 'papers': 1, 'many': 1, 'students': 1, 'define': 1, 'in': 6, 'terms': 1, 'length': 2, 'a': 15, 'paragraph': 8, 'is': 6, 'group': 3, 'at': 1, 'least': 1, 'five': 1, 'sentences': 4, 'half': 1, 'page': 1, 'long': 2, 'etc': 1, 'reality': 1, 'though': 1, 'unity': 1, 'and': 2, 'coherence': 1, 'ideas': 1, 'among': 1, 'what': 2, 'constitutes': 1, 'defined': 1, 'as': 2, 'or': 2, 'single': 1, 'sentence': 3, 'that': 2, 'forms': 1, 'unit': 1, 'appearance': 1, 'do': 1, 'not': 1, 'determine': 1, 'whether': 1, 'section': 1, 'paper': 1, 'for': 1, 'instance': 1, 'some': 1, 'styles': 2, 'writing': 1, 'particularly': 1, 'journalistic': 1, 'can': 1, 'be': 1, 'just': 1, 'one': 2, 'ultimately': 1, 'support': 1, 'main': 1, 'idea': 2, 'this': 2, 'handout': 1, 'we': 1, 'will': 1, 'refer': 1, 'to': 1, 'controlling': 1, 'because': 1, 'it': 1, 'controls': 1, 'happens': 1, 'rest': 1} \n",
      "\n",
      "table2Gram \n",
      " {'paragraphs are': 1, 'are the': 1, 'the building': 1, 'building blocks': 1, 'blocks of': 1, 'of papers': 1, 'many students': 1, 'students define': 1, 'define paragraphs': 1, 'paragraphs in': 1, 'in terms': 1, 'terms of': 1, 'of length': 1, 'length a': 1, 'a paragraph': 7, 'paragraph is': 4, 'is a': 3, 'a group': 2, 'group of': 3, 'of at': 1, 'at least': 1, 'least five': 1, 'five sentences': 1, 'is half': 1, 'half a': 1, 'a page': 1, 'page long': 1, 'in reality': 1, 'the unity': 1, 'unity and': 1, 'and coherence': 1, 'coherence of': 1, 'of ideas': 1, 'ideas among': 1, 'among sentences': 1, 'sentences is': 1, 'is what': 1, 'what constitutes': 1, 'constitutes a': 1, 'is defined': 1, 'defined as': 1, 'as a': 1, 'of sentences': 2, 'sentences or': 1, 'or a': 1, 'a single': 1, 'single sentence': 1, 'sentence that': 1, 'that forms': 1, 'forms a': 1, 'a unit': 1, 'length and': 1, 'and appearance': 1, 'appearance do': 1, 'do not': 1, 'not determine': 1, 'determine whether': 1, 'whether a': 1, 'a section': 1, 'section in': 1, 'in a': 1, 'a paper': 1, 'paper is': 1, 'for instance': 1, 'in some': 1, 'some styles': 1, 'styles of': 1, 'of writing': 1, 'particularly journalistic': 1, 'journalistic styles': 1, 'paragraph can': 1, 'can be': 1, 'be just': 1, 'just one': 1, 'one sentence': 1, 'sentence long': 1, 'a sentence': 1, 'sentence or': 1, 'or group': 1, 'sentences that': 1, 'that support': 1, 'support one': 1, 'one main': 1, 'main idea': 1, 'in this': 1, 'this handout': 1, 'we will': 1, 'will refer': 1, 'refer to': 1, 'to this': 1, 'this as': 1, 'as the': 1, 'the controlling': 1, 'controlling idea': 1, 'because it': 1, 'it controls': 1, 'controls what': 1, 'what happens': 1, 'happens in': 1, 'in the': 1, 'the rest': 1, 'rest of': 1, 'of the': 1, 'the paragraph': 1} \n",
      "\n",
      "table3Gram \n",
      " {'paragraphs are the': 1, 'are the building': 1, 'the building blocks': 1, 'building blocks of': 1, 'blocks of papers': 1, 'many students define': 1, 'students define paragraphs': 1, 'define paragraphs in': 1, 'paragraphs in terms': 1, 'in terms of': 1, 'terms of length': 1, 'of length a': 1, 'length a paragraph': 1, 'a paragraph is': 4, 'paragraph is a': 2, 'is a group': 1, 'a group of': 2, 'group of at': 1, 'of at least': 1, 'at least five': 1, 'least five sentences': 1, 'paragraph is half': 1, 'is half a': 1, 'half a page': 1, 'a page long': 1, 'the unity and': 1, 'unity and coherence': 1, 'and coherence of': 1, 'coherence of ideas': 1, 'of ideas among': 1, 'ideas among sentences': 1, 'among sentences is': 1, 'sentences is what': 1, 'is what constitutes': 1, 'what constitutes a': 1, 'constitutes a paragraph': 1, 'paragraph is defined': 1, 'is defined as': 1, 'defined as a': 1, 'as a group': 1, 'group of sentences': 2, 'of sentences or': 1, 'sentences or a': 1, 'or a single': 1, 'a single sentence': 1, 'single sentence that': 1, 'sentence that forms': 1, 'that forms a': 1, 'forms a unit': 1, 'length and appearance': 1, 'and appearance do': 1, 'appearance do not': 1, 'do not determine': 1, 'not determine whether': 1, 'determine whether a': 1, 'whether a section': 1, 'a section in': 1, 'section in a': 1, 'in a paper': 1, 'a paper is': 1, 'paper is a': 1, 'is a paragraph': 1, 'in some styles': 1, 'some styles of': 1, 'styles of writing': 1, 'particularly journalistic styles': 1, 'a paragraph can': 1, 'paragraph can be': 1, 'can be just': 1, 'be just one': 1, 'just one sentence': 1, 'one sentence long': 1, 'is a sentence': 1, 'a sentence or': 1, 'sentence or group': 1, 'or group of': 1, 'of sentences that': 1, 'sentences that support': 1, 'that support one': 1, 'support one main': 1, 'one main idea': 1, 'in this handout': 1, 'we will refer': 1, 'will refer to': 1, 'refer to this': 1, 'to this as': 1, 'this as the': 1, 'as the controlling': 1, 'the controlling idea': 1, 'because it controls': 1, 'it controls what': 1, 'controls what happens': 1, 'what happens in': 1, 'happens in the': 1, 'in the rest': 1, 'the rest of': 1, 'rest of the': 1, 'of the paragraph': 1} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build gram table\n",
    "table1Gram = {}\n",
    "table2Gram = {}\n",
    "table3Gram = {}\n",
    "\n",
    "def increment(dictionary, word):\n",
    "    if word in dictionary:\n",
    "        dictionary[word] = dictionary[word] + 1\n",
    "    else:\n",
    "        dictionary[word] = 1\n",
    "        \n",
    "# get ngram table\n",
    "for line in lines:\n",
    "    m = len(line)\n",
    "    for index, word in enumerate(line):\n",
    "        increment(table1Gram,word)\n",
    "        if index < (m - 1):\n",
    "            increment(table2Gram, word + \" \" + line[index+1])\n",
    "        if index < (m - 2):\n",
    "            increment(table3Gram, word + \" \" + line[index+1] + \" \" + line[index+2])\n",
    "            \n",
    "print('table1Gram \\n', table1Gram , '\\n')\n",
    "print('table2Gram \\n', table2Gram , '\\n')\n",
    "print('table3Gram \\n', table3Gram , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Discounting Interpolation\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P_{ad}(w_i | w_{i-1}) & = \\frac{c(w_{i-1},w_i)-d}{c(w_{i-1}} + \\lambda(w_{i-1})P(w) \\\\ \n",
    "& = \\frac{discounted \\ bigram}{sth} + interpolation \\ weight * unigram\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where d could be 0.75 from experience\n",
    "\n",
    "### Kneser-Ney Smoothing\n",
    "\n",
    "Change: **P(w)** \"how likely is w\" => **P_continuation(w)** \"how likely is w to appear as a novel continuation\"\n",
    "\n",
    "\n",
    "#### bigram formulation\n",
    "\n",
    "$$\n",
    "P_{KN}(w_i | w_{i-1}) = \\frac{max(c(w_{i-1}, w_i) -d, 0)}{c(w_{i-1})}  + \\lambda(w_{i-1}) P_{continuation}(w_i) \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\lambda(w_{i-1}) & = \\frac{d}{c(w_{i-1})}|\\{w: c(w_{i-1},w) > 0 \\}| \\\\\n",
    "& = the \\ normalized \\ discount * the \\ number \\ of \\ word \\ types \\ that \\ follow \\ w_{i-1} \\\\\n",
    "& = low \\ order \\ weight \\ or \\ backoff \\ weight\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "P_{continuation}(w_i) & = \\frac{|\\{w: c(w,w_{i}) > 0 \\}|}{|\\{(w_{j-1},w_j): c(w_{j-1},w_j) > 0 \\}|} \\\\\n",
    "& = \\frac{number \\ when \\ word \\ w_{i} \\ is \\ a \\ novel \\ continuation}{total \\ number \\ of \\ word \\ bigram \\ types} \\\\\n",
    "& = low \\ order \\ probability\n",
    "\\end{split}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13221153846153846\n",
      "0.04326923076923077\n"
     ]
    }
   ],
   "source": [
    "def p_kn_2(pre_word, current_word, d = 0.75):\n",
    "    \n",
    "    w_i_1_set = [key for key in list(table2Gram.keys()) if key.split(\" \")[0] == pre_word]\n",
    "    w_i_set = [key for key in list(table2Gram.keys()) if key.split(\" \")[1] == current_word]\n",
    "\n",
    "    low_order_weight = d / table1Gram[pre_word] * len(w_i_1_set)\n",
    "    p_continuation = len(w_i_set) / len(table2Gram.keys())\n",
    "    \n",
    "    w_i_1_w_i = pre_word + \" \" + current_word\n",
    "    c = table2Gram[w_i_1_w_i] if w_i_1_w_i in table2Gram else 0\n",
    "    return max(c-d, 0) / table1Gram[pre_word] + low_order_weight * p_continuation\n",
    "\n",
    "print(p_kn_2('paragraphs','are'))\n",
    "print(p_kn_2('paragraphs','of'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general recursive formulation\n",
    "\n",
    "$$\n",
    "P_{KN}(w_i | w_{i-n+1}^{i-1}) = \\frac{max(C_{KN}(w_{i-n+1}^{i}) -d, 0) }{C_{KN}(w_{i-n+1}^{i-1})}  + \\lambda(w_{i-n+1}^{i-1}) P_{KN}(w_i | w_{i-n+2}^{i-1}) \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$ \n",
    "C_{KN}(\\bullet) = \\left \\{\n",
    "\\begin{aligned}\n",
    "    count(\\bullet) \\ for \\ the \\ highest \\ order \\\\\n",
    "    continutation \\ count(\\bullet) \\ for \\ lower \\  order \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "$\n",
    "w_{i-n+1}^{i-1} \\ denotes \\ word \\ w_{i-n+1}w_{i-n+2}\\dots w_{i-1}\n",
    "$\n",
    "\n",
    "#### modified Kneser-Ney smoothing\n",
    "\n",
    "$$\n",
    "P_{KN}(w_i | w_{i-n+1}^{i-1}) = \n",
    "\\frac\n",
    "    {C(w_{i-n+1}^{i}) - D(C(w_{i-n+1}^{i}))}\n",
    "    {\\sum_{w_i} C(w_{i-n+1}^i)}  + \\lambda(w_{i-n+1}^{i-1}) P_{KN}(w_i | w_{i-n+2}^{i-1}) \n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "D(c) = \\left \\{\n",
    "\\begin{aligned}\n",
    "    & 0 \\ & if \\ c=0 \\\\\n",
    "    & D_1 \\ & if \\ c=1 \\\\\n",
    "    & D_2 \\ & if \\ c=2 \\\\\n",
    "    & D_{3+} \\ & if \\ c\\geq3 \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda(w_{i-n+1}^{i-1}) = \n",
    "\\frac\n",
    "    {D_1N_1(w_{i-n+1}^{i-1}\\cdot)+D_2N_2(w_{i-n+1}^{i-1}\\cdot)+D_{3+}N_{3+}(w_{i-n+1}^{i-1}\\cdot)}\n",
    "    {\\sum_{w_i}c(w_{i-n+1}^i)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "N_{1+}(\\bullet w_{i-n+2}^i) = |\\{w_{i-n+1} : C(w_{i-n+1}^i > 0\\}|\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
