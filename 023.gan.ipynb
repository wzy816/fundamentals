{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.backend import *\n",
    "from tensorflow.keras.optimizers import * \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flags\n",
    "img_size = 28 # mnist image width/height\n",
    "\n",
    "seed_size = 100 # size of randomly seed\n",
    "kernel_size = 5\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, _),(_,_) = mnist.load_data()\n",
    "\n",
    "x_train = (x_train.reshape(x_train.shape[0],img_size,img_size,1).astype('float32') -127.5) / 255.\n",
    "\n",
    "def next_batch():\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        yield x_train[i:i + batch_size]\n",
    "        \n",
    "total_batch = len(x_train)/batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "# that generates an image from a random noise\n",
    "\n",
    "generator = tf.keras.Sequential([\n",
    "    Dense(int(img_size/4 * img_size/4 * 256),\n",
    "          use_bias=False, input_shape=(seed_size,)),\n",
    "#     BatchNormalization(),\n",
    "#     LeakyReLU(),\n",
    "    Reshape((int(img_size/4),int(img_size/4), 256)),\n",
    "    \n",
    "    Conv2DTranspose(filters=128,\n",
    "                    kernel_size=(kernel_size,kernel_size),\n",
    "                    strides=1,\n",
    "                    padding = 'same',\n",
    "                    use_bias=False),\n",
    "#     BatchNormalization(),\n",
    "#     LeakyReLU(),\n",
    "    \n",
    "    Conv2DTranspose(filters=64,\n",
    "                kernel_size=(kernel_size,kernel_size),\n",
    "                strides=2,\n",
    "                padding = 'same',\n",
    "                use_bias=False),\n",
    "#     BatchNormalization(),\n",
    "#     LeakyReLU(),\n",
    "    \n",
    "    Conv2DTranspose(filters=1,\n",
    "            kernel_size=(kernel_size,kernel_size),\n",
    "            strides= 2,\n",
    "            padding = 'same',\n",
    "            use_bias=False,\n",
    "            activation='tanh')\n",
    "])\n",
    "\n",
    "for layer in generator.layers:\n",
    "    print(layer.input_shape, layer.output_shape)\n",
    "    \n",
    "print(generator.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator\n",
    "# that classifiy generated/real image as 1(real) or 2(fake)\n",
    "discriminator = tf.keras.Sequential([\n",
    "    Conv2D(\n",
    "        filters = 64,\n",
    "        kernel_size=(kernel_size,kernel_size),\n",
    "        strides= 2,\n",
    "        padding = 'same',\n",
    "        input_shape=(28,28,1)),\n",
    "#     LeakyReLU(),\n",
    "#     Dropout(0.3),\n",
    "    \n",
    "    Conv2D(\n",
    "        filters = 128,\n",
    "        kernel_size=(kernel_size,kernel_size),\n",
    "        strides= 2,\n",
    "        padding = 'same'),\n",
    "#     LeakyReLU(),\n",
    "#     Dropout(0.3),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "for layer in discriminator.layers:\n",
    "    print(layer.input_shape, layer.output_shape)\n",
    "\n",
    "print(discriminator.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run image generation once\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    seed = tf.random.normal([1, seed_size])\n",
    "    image = generator(seed, training=False)\n",
    "    img = image.eval(session=sess)\n",
    "    \n",
    "    plt.imshow(img[0,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def generator_loss(fake):\n",
    "    all_zeros = tf.zeros_like(fake)\n",
    "    return binary_crossentropy(all_zeros, fake)\n",
    "\n",
    "def discriminator_loss(fake, real):\n",
    "    all_zeros = tf.zeros_like(fake)\n",
    "    all_ones = tf.ones_like(real)\n",
    "    return binary_crossentropy(all_zeros, fake) + binary_crossentropy(all_ones, real)\n",
    "\n",
    "# optimizer\n",
    "generator_optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "discriminator_optimizer = tf.train.AdagradOptimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        i = 0\n",
    "        for batch in next_batch():\n",
    "            if i % 100 == 0:\n",
    "                print('epoch', epoch, \"batch\", i, '/', total_batch)\n",
    "            \n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                real = discriminator(batch, training = True)\n",
    "\n",
    "                noise = tf.random.normal([batch_size, seed_size])\n",
    "                generated = generator(noise, training = True)\n",
    "                fake = discriminator(generated, training = True)\n",
    "\n",
    "                gen_loss = generator_loss(fake)\n",
    "                disc_loss = discriminator_loss(fake, real)\n",
    "\n",
    "                gen_gradients = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
    "                disc_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "                       \n",
    "                generator_optimizer.apply_gradients(zip(gen_gradients,generator.trainable_variables))\n",
    "                discriminator_optimizer.apply_gradients(zip(disc_gradients,discriminator.trainable_variables))\n",
    "                i = i+1\n",
    "                \n",
    "        # run image generation \n",
    "        # once at the end of each epoch\n",
    "        images = generator(noise, training=False).eval(session=sess)\n",
    "        \n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(10,10,i+1)\n",
    "            plt.imshow(images[i,:,:,0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
