{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.backend import *\n",
    "from tensorflow.keras.optimizers import * \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags\n",
    "img_size = 28 # mnist image width/height\n",
    "\n",
    "seed_size = 100 # size of randomly seed\n",
    "kernel_size = 5 # conv kernal\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, _),(_,_) = mnist.load_data()\n",
    "\n",
    "x_train = (x_train.reshape(x_train.shape[0], img_size, img_size, 1).astype('float32') - 127.5) / 255.\n",
    "\n",
    "total_batch = len(x_train) / batch_size\n",
    "\n",
    "def next_batch():\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        yield x_train[i:i + batch_size]\n",
    "        \n",
    "def next_sample(batch_size):\n",
    "    d = np.random.normal(0, 1.0, (batch_size, seed_size)).astype('float32')\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test next_batch and next_sample\n",
    "\n",
    "# bs = [i for i in next_batch()][0]\n",
    "# for i in bs[0:10]:\n",
    "#     plt.figure()\n",
    "#     plt.imshow(i[:,:,0]*255+127.5)\n",
    "# plt.show()\n",
    "\n",
    "# print(next_sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = tf.keras.Sequential([\n",
    "    Dense(7*7*batch_size, use_bias=False, input_shape=(seed_size,)),\n",
    "    BatchNormalization(),LeakyReLU(),\n",
    "    Reshape((7,7,batch_size)),\n",
    "    Conv2DTranspose(filters=128,\n",
    "                    kernel_size=(kernel_size,kernel_size),\n",
    "                    strides=1,\n",
    "                    padding = 'same',\n",
    "                    use_bias=False),\n",
    "    BatchNormalization(),LeakyReLU(),\n",
    "    Conv2DTranspose(filters=64,\n",
    "                    kernel_size=(kernel_size,kernel_size),\n",
    "                    strides=2,\n",
    "                    padding = 'same',\n",
    "                    use_bias=False),\n",
    "    BatchNormalization(),LeakyReLU(),\n",
    "    Conv2DTranspose(filters=1,\n",
    "                    kernel_size=(kernel_size,kernel_size),\n",
    "                    strides=2,\n",
    "                    padding = 'same',\n",
    "                    use_bias=False),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test image generation\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     z = next_sample(1)\n",
    "#     image = generator(z, training=False).eval(session=sess)    \n",
    "#     plt.figure()\n",
    "#     plt.imshow(image[0,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiy generated/train image as 1(real) or 0(fake)\n",
    "discriminator = tf.keras.Sequential([\n",
    "    Conv2D(filters = 64,\n",
    "        kernel_size=(kernel_size,kernel_size),\n",
    "        strides= 2,\n",
    "        padding = 'same',\n",
    "        input_shape=(28,28,1)),\n",
    "    LeakyReLU(),\n",
    "    Dropout(0.3),\n",
    "    Conv2D(filters = 128,\n",
    "        kernel_size=(kernel_size,kernel_size),\n",
    "        strides= 2,\n",
    "        padding = 'same'),\n",
    "    LeakyReLU(),\n",
    "    Dropout(0.3),  \n",
    "    Flatten(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return binary_crossentropy(tf.ones_like(fake), fake, from_logits=True)\n",
    "\n",
    "def discriminator_loss(fake, real):\n",
    "    return binary_crossentropy(tf.zeros_like(fake), fake,from_logits=True) + binary_crossentropy(tf.ones_like(real), real, from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test g and d\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     samples = tf.placeholder(tf.float32,[None,100])\n",
    "#     generated = generator(samples)\n",
    "#     fake = discriminator(generated)\n",
    "    \n",
    "#     images = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "#     real = discriminator(images) \n",
    "    \n",
    "#     g_loss = generator_loss(fake)\n",
    "#     d_loss = discriminator_loss(fake,real)\n",
    "    \n",
    "#     z = next_sample(10)\n",
    "#     f,r,g,d = sess.run([fake,real, g_loss, d_loss], feed_dict={samples:z, images:x_train[0:10]})\n",
    "#     print(f,'\\n')\n",
    "#     print(r,'\\n')\n",
    "#     print(g,'\\n')\n",
    "#     print(d,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "    \n",
    "samples = tf.placeholder(tf.float32,[None,100])\n",
    "generated = generator(samples)\n",
    "fake = discriminator(generated)\n",
    "\n",
    "images = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "real = discriminator(images) \n",
    "\n",
    "g_loss = generator_loss(fake)\n",
    "d_loss = discriminator_loss(fake,real)    \n",
    "\n",
    "g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss,var_list = generator.trainable_variables)\n",
    "d_opt = tf.train.AdagradOptimizer(learning_rate).minimize(d_loss,var_list = discriminator.trainable_variables)\n",
    "\n",
    "# train\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    test_sample = next_sample(100)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('epoch',epoch)\n",
    "        \n",
    "        for x in next_batch():\n",
    "            z = next_sample(batch_size)\n",
    "            _,g_l = sess.run([g_opt, g_loss], feed_dict= {\n",
    "                samples:z,\n",
    "            })\n",
    "            _,d_l = sess.run([d_opt, d_loss], feed_dict= {\n",
    "                samples:z,\n",
    "                images:x\n",
    "            })\n",
    "            \n",
    "            \n",
    "    image = sess.run([generated], feed_dict={samples:test_sample})\n",
    "    image = np.array(image) * 255 + 127.5\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        j = choice(range(batch_size))\n",
    "        plt.imshow(image[0,j,:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
